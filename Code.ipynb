{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357e2491-8f79-4d14-818f-90fe687d6505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Library\n",
    "\n",
    "import os\n",
    "import base64\n",
    "import sqlite3\n",
    "import GEOparse\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import seaborn as sns\n",
    "from io import BytesIO\n",
    "from flask import send_file\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "from scipy import sparse\n",
    "from keras.layers import Input\n",
    "import category_encoders as ce\n",
    "from scipy.sparse import hstack\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "from flask import Flask, render_template, request, redirect, url_for\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0d4b6f-2991-4a28-9624-1ab584f710d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "# Home page route\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return render_template(\"home.html\")\n",
    "\n",
    "@app.route(\"/about\")\n",
    "def about():\n",
    "    return render_template(\"about.html\")\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "def create_table():\n",
    "    conn = sqlite3.connect(\"contacts.db\")\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"CREATE TABLE IF NOT EXISTS contacts (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT, email TEXT, message TEXT)\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "@app.route(\"/contact\", methods=[\"GET\", \"POST\"])\n",
    "def contact():\n",
    "    if request.method == \"POST\":\n",
    "        name = request.form.get(\"name\")\n",
    "        email = request.form.get(\"email\")\n",
    "        message = request.form.get(\"message\")\n",
    "        \n",
    "        # Save the data to the database\n",
    "        conn = sqlite3.connect(\"contacts.db\")\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"INSERT INTO contacts (name, email, message) VALUES (?, ?, ?)\", (name, email, message))\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    return render_template(\"contact.html\")\n",
    "\n",
    "@app.route(\"/submit-contact\", methods=[\"POST\"])\n",
    "def submit_contact():\n",
    "    name = request.form.get(\"name\")\n",
    "    email = request.form.get(\"email\")\n",
    "    message = request.form.get(\"message\")\n",
    "\n",
    "    # Save the data to the database\n",
    "    conn = sqlite3.connect(\"contacts.db\")\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"INSERT INTO contacts (name, email, message) VALUES (?, ?, ?)\", (name, email, message))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "    return redirect(url_for(\"contact\"))\n",
    "\n",
    "@app.route(\"/faq\")\n",
    "def faq():\n",
    "    return render_template(\"faq.html\")\n",
    "\n",
    "@app.route(\"/program\", methods=[\"GET\", \"POST\"])\n",
    "def program():\n",
    "    if request.method == \"POST\":\n",
    "        input_data = request.form.get(\"input_data\")  # Get the input data from the form\n",
    "        file_data = None\n",
    "\n",
    "        if \"file_upload\" in request.files:\n",
    "            file = request.files[\"file_upload\"]\n",
    "            if file.filename != \"\":\n",
    "                # Process the uploaded file as needed\n",
    "                file_data = file.read()  # Read the contents of the file\n",
    "\n",
    "        # Perform your program logic using input_data or file_data\n",
    "        result = None\n",
    "        if input_data or file_data:\n",
    "            result = \"Running program\"\n",
    "            # Perform your program logic here using input_data or file_data\n",
    "            if input_data:\n",
    "                result += f\" with input data: {input_data}\"\n",
    "            if file_data:\n",
    "                result += \" with uploaded file\"\n",
    "        # Introduce a delay of 5 seconds\n",
    "        time.sleep(3)\n",
    "        # Render the result template with the result value\n",
    "        return render_template(\"result.html\", result=result, accuracy=accuracy)\n",
    "\n",
    "    return render_template(\"program.html\")\n",
    "    \n",
    "@app.route(\"/visualization\")\n",
    "def visualization():\n",
    "    return render_template(\"visualization.html\")\n",
    "    \n",
    "@app.route(\"/visualization/prc\")\n",
    "def prc():\n",
    "    # Generate predictions on test data\n",
    "    y_pred = model.predict(X_test_encoded)\n",
    "\n",
    "    # Compute precision and recall\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "\n",
    "    # Compute Area Under the Curve (AUC) for PRC\n",
    "    prc_auc = auc(recall, precision)\n",
    "\n",
    "    # Plot PRC curve\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(recall, precision, color='darkorange', lw=2, label='PRC curve (area = %0.2f)' % prc_auc)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve (PRC)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    buffer = BytesIO()\n",
    "    plt.savefig(buffer, format='png')\n",
    "    buffer.seek(0)\n",
    "    image_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "\n",
    "    # Render the prc.html template with the image\n",
    "    return render_template(\"prc.html\", image_base64=image_base64)\n",
    "\n",
    "@app.route(\"/visualization/roc\")\n",
    "def roc():\n",
    "    # Generate predictions on test data\n",
    "    y_pred = model.predict(X_test_encoded)\n",
    "\n",
    "    # Compute false positive rate and true positive rate\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "\n",
    "    # Compute Area Under the Curve (AUC) for ROC\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    # Save the plot as an image\n",
    "    buffer = BytesIO()\n",
    "    plt.savefig(buffer, format='png')\n",
    "    buffer.seek(0)\n",
    "    image_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "\n",
    "    # Render the roc.html template with the image\n",
    "    return render_template(\"roc.html\", image_base64=image_base64)\n",
    "\n",
    "@app.route(\"/visualization/bar\")\n",
    "def bar():\n",
    "    # Set the figure size\n",
    "    plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Select the top 10 most frequent gene_assignment values\n",
    "    top_10_gene_assignments = df_subset['gene_assignment'].value_counts().nlargest(10).index\n",
    "# Filter the dataframe for the top 10 gene_assignment values\n",
    "    df_top_10 = df_subset[df_subset['gene_assignment'].isin(top_10_gene_assignments)]\n",
    "# Calculate the average expression levels for each unique gene_assignment\n",
    "    average_expression_by_gene_assignment = df_top_10.groupby('gene_assignment')['VALUE'].mean().reset_index()\n",
    "\n",
    "# Sort the dataframe by average expression in descending order\n",
    "    average_expression_by_gene_assignment = average_expression_by_gene_assignment.sort_values('VALUE', ascending=False)\n",
    "\n",
    "# Create the bar plot\n",
    "    ax = sns.barplot(x='gene_assignment', y='VALUE', data=average_expression_by_gene_assignment)\n",
    "\n",
    "# Set the x-axis tick labels to the shortened names\n",
    "    shortened_names = average_expression_by_gene_assignment['gene_assignment'].apply(lambda x: x.split(\" // \", 2)[:2]).apply(lambda x: \" // \".join(x))\n",
    "    ax.set_xticklabels(shortened_names, rotation=45, ha='right')\n",
    "\n",
    "# Set the title and labels\n",
    "    plt.title('Top 10 Most Frequent Gene Assignments (Average Expression)')\n",
    "    plt.xlabel('Gene Assignments')\n",
    "    plt.ylabel('Average Expression')\n",
    "    plt.subplots_adjust(bottom=0.4)\n",
    "\n",
    "    # Save the plot as an image\n",
    "    buffer = BytesIO()\n",
    "    plt.savefig(buffer, format='png')\n",
    "    buffer.seek(0)\n",
    "    image_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "\n",
    "    # Render the bar.html template with the image\n",
    "    return render_template(\"bar.html\", image_base64=image_base64)\n",
    "\n",
    "@app.route(\"/visualization/trainval\")\n",
    "def trainval():\n",
    "\n",
    "    plt.figure(figsize=(12, 5.5))\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "# Save the plot as an image\n",
    "    buffer = BytesIO()\n",
    "    plt.savefig(buffer, format='png')\n",
    "    buffer.seek(0)\n",
    "    image_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "\n",
    "# Show the plot (optional)\n",
    "    plt.show()\n",
    "    return render_template(\"trainval.html\", image_base64=image_base64)\n",
    "\n",
    "\n",
    "@app.route(\"/performance\")\n",
    "def performance():\n",
    "# Generate predictions on test data\n",
    "    y_pred = model.predict(X_test_encoded)s\n",
    "\n",
    "# Convert predictions to binary values (0 or 1) based on a threshold (e.g., 0.5)\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Calculate accuracy\n",
    "    accuracy = round(accuracy_score(y_test, y_pred_binary), 3)\n",
    "# Calculate precision\n",
    "    precision = round(precision_score(y_test, y_pred_binary, zero_division=1), 3)\n",
    "# Calculate sensitivity (recall)\n",
    "    sensitivity = round(recall_score(y_test, y_pred_binary), 3)\n",
    "# Calculate F1 score\n",
    "    f1 = round(f1_score(y_test, y_pred_binary), 3)\n",
    "# Calculate log loss\n",
    "    logloss = round(log_loss(y_test, y_pred), 3)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred_binary)    \n",
    "    #for row in cm:\n",
    "     #   print(' '.join([str(elem) for elem in row]))\n",
    "    return render_template(\"performance.html\", accuracy=accuracy, precision=precision, sensitivity=sensitivity, f1=f1, logloss=logloss, cm=cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f912ea2-cd7b-4f3f-8f13-99fe4dc0a67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    create_table()\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0251809b-bd25-41e2-a8d2-511394f89240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is running the entire code in one go \n",
    "\n",
    "# Initialization\n",
    "# Define the path where the data will be stored\n",
    "data_dir = \".\"\n",
    "\n",
    "# Define the GEO accession number\n",
    "geo_accession = \"GSE67311\"\n",
    "\n",
    "# Define the full path to the expected file\n",
    "expected_file_path = os.path.join(data_dir, f\"{geo_accession}_family.soft.gz\")\n",
    "\n",
    "# Check if the file already exists\n",
    "if not os.path.isfile(expected_file_path):\n",
    "    # If the file does not exist, download the data\n",
    "    print(f\"Downloading {geo_accession} data...\")\n",
    "    url = f\"https://ftp.ncbi.nlm.nih.gov/geo/series/GSE67nnn/{geo_accession}/soft/{geo_accession}_family.soft.gz\"\n",
    "    urllib.request.urlretrieve(url, expected_file_path)\n",
    "    #gse = GEOparse.get_GEO(geo=geo_accession, destdir=data_dir)\n",
    "    print(f\"Data for {geo_accession} downloaded successfully.\")\n",
    "else:\n",
    "    # If the file exists, load it from the file\n",
    "    print(f\"Data for {geo_accession} already downloaded. Loading from file...\")\n",
    "    \n",
    "gse = GEOparse.get_GEO(filepath=expected_file_path)\n",
    "# Create an empty DataFrame to append expression data\n",
    "expression_data = pd.DataFrame()\n",
    "\n",
    "# Find the corresponding rows in the second table based on ID_REF values\n",
    "platform_list = list(gse.gpls.values())\n",
    "platform = platform_list[0]\n",
    "info_data = platform.table\n",
    "# Loop through each GSM object, extract the expression data, and append it to the DataFrame\n",
    "for gsm_name, gsm in gse.gsms.items():\n",
    "    # Extract expression data for the current GSM object\n",
    "    gsm_expression_data = pd.DataFrame(gsm.table)\n",
    "    mapped_info = info_data[info_data['ID'].isin(gsm_expression_data['ID_REF'])]\n",
    "    # Add the \"GSM\" column\n",
    "    gsm_expression_data['GSM'] = gsm_name\n",
    "    # Merge the mapped information with the expression data\n",
    "    gsm_expression_data = pd.merge(gsm_expression_data, mapped_info, left_on='ID_REF', right_on='ID', how='left')\n",
    "    # Append the expression data to the main DataFrame\n",
    "    expression_data = pd.concat([expression_data, gsm_expression_data], ignore_index=True)\n",
    "\n",
    "numeric_columns = expression_data.drop(columns=['SPOT_ID', 'ID'])\n",
    "expression_data = numeric_columns.reindex(columns=['ID_REF', 'VALUE'] + numeric_columns.columns.drop(['ID_REF', 'VALUE']).tolist())\n",
    "# Replace '---' values with NaN\n",
    "expression_data.replace('---', np.nan, inplace=True)\n",
    "# Remove rows with blank 'gene_assignment' values\n",
    "expression_data = expression_data.dropna(subset=['gene_assignment'])\n",
    "expression_data = expression_data.dropna(subset=['VALUE'])\n",
    "\n",
    "# Group the dataframe by \"gene_assignment\" column\n",
    "grouped = expression_data.groupby(\"gene_assignment\")\n",
    "\n",
    "# Define the outlier threshold (e.g., 3 standard deviations)\n",
    "threshold = 2\n",
    "\n",
    "# Calculate z-scores within each group using the transform function\n",
    "expression_data['z_score'] = grouped[\"VALUE\"].transform(lambda x: (x - x.mean()) / x.std())\n",
    "\n",
    "# Identify rows with z-scores beyond the threshold\n",
    "outlier_mask = np.abs(expression_data['z_score']) > threshold\n",
    "\n",
    "# Set the score as 1 for outlier rows and 2 for non-outlier rows\n",
    "expression_data['score'] = np.where(outlier_mask, 1, 2)\n",
    "\n",
    "# Filter out the outlier rows\n",
    "expression_data = expression_data[expression_data['score'] != 1]\n",
    "\n",
    "# Reset the index of the updated dataframe\n",
    "expression_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Drop the \"z_score\" and \"score\" columns\n",
    "expression_data = expression_data.copy()\n",
    "expression_data.drop([\"z_score\", \"score\"], axis=1, inplace=True)\n",
    "\n",
    "# Extract the \"VALUE\" column\n",
    "values = expression_data[\"VALUE\"]\n",
    "\n",
    "# Normalize the \"VALUE\" column\n",
    "scaler = StandardScaler()\n",
    "values_scaled = scaler.fit_transform(values.values.reshape(-1, 1))\n",
    "\n",
    "# Create a new DataFrame with the normalized \"VALUE\" column\n",
    "df_scaled = expression_data.copy()\n",
    "df_scaled[\"VALUE\"] = values_scaled\n",
    "\n",
    "labels = gse.phenotype_data.set_index('geo_accession')['characteristics_ch1.0.diagnosis']\n",
    "df_scaled['labels'] = df_scaled['GSM'].map(labels)\n",
    "# Drop any rows in 'df_scaled' that do not have a label due to missing mapping\n",
    "df_scaled.dropna(subset=['labels'], inplace=True)\n",
    "# Extract the labels into a separate variable and drop the 'GSM' and 'labels' columns from 'df_scaled'\n",
    "final_labels = df_scaled['labels'].values\n",
    "\n",
    "class_order = [\"healthy control\", \"fibromyalgia\"]\n",
    "# Initialize the label encoder\n",
    "final_labels_2d = final_labels.reshape(-1, 1)\n",
    "ordinal_encoder = OrdinalEncoder(categories=[class_order])\n",
    "\n",
    "# Fit label encoder and return encoded labels\n",
    "encoded_labels = ordinal_encoder.fit_transform(final_labels_2d).flatten()\n",
    "df_scaled_shuffled, encoded_labels_shuffled = shuffle(df_scaled, encoded_labels, random_state=42)\n",
    "\n",
    "# Select a subset of the data\n",
    "sample_size = int(len(df_scaled_shuffled) * 0.1)\n",
    "df_subset = df_scaled_shuffled[:sample_size]\n",
    "labels_subset = encoded_labels_shuffled[:sample_size]\n",
    "    # Machine Learning Model\n",
    "df_subset = df_subset.drop(\"GSM\", axis=1)\n",
    "# Split the data into train and test sets\n",
    "X_train, x_test, Y_train, y_test = train_test_split(df_subset, labels_subset, test_size=0.2, random_state=42)\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Apply label encoding to the desired columns\n",
    "X_train_encoded = X_train.copy()  # Make a copy of the original DataFrame\n",
    "X_test_encoded = x_test.copy()\n",
    "\n",
    "columns_to_encode = [\"GB_LIST\", \"seqname\", \"RANGE_GB\", \"RANGE_STRAND\", \"gene_assignment\", \"mrna_assignment\", \"labels\", \"category\"]\n",
    "for column in columns_to_encode:\n",
    "    X_train_encoded[column] = label_encoder.fit_transform(X_train_encoded[column])\n",
    "    X_test_encoded[column] = label_encoder.fit_transform(X_test_encoded[column])\n",
    "\n",
    "# Reverse the encoding for the 'labels' column\n",
    "X_train_encoded['labels'] = 1 - X_train_encoded['labels']\n",
    "X_test_encoded['labels'] = 1 - X_test_encoded['labels']\n",
    "\n",
    "X_train_encoded = np.array(X_train_encoded.values, dtype=np.float32)\n",
    "X_test_encoded = np.array(X_test_encoded.values, dtype=np.float32)\n",
    "# Define the neural network structure\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(X_train_encoded.shape[1],)))\n",
    "model.add(Dense(256, activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(X_train_encoded, Y_train, validation_data=(X_test_encoded, y_test), epochs=40, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_encoded, y_test)\n",
    "print(f'Test loss: {loss}, Test accuracy: {accuracy}')\n",
    "accuracy = round(accuracy * 100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61114035-1297-47d5-96a5-f762f25523bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sqlite3\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect(\"contacts.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Execute a SELECT query to retrieve all rows from the \"contacts\" table\n",
    "cursor.execute(\"SELECT * FROM contacts\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Print the table header\n",
    "header = [description[0] for description in cursor.description]\n",
    "print(header)\n",
    "\n",
    "# Print each row of the table\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6068931f-c513-49f0-acc4-0e6710be2620",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
